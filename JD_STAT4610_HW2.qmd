---
title: "JD_STAT4610_HW2"
author: "John Dennis"
format: pdf
---

### Problem 1

We're told that

$$
\hat{\beta}_{ridge} = (\textbf{X}^T\textbf{X}+\lambda \mathbf{I})^{-1}\textbf{X}^T\mathbf{Y}, \ \ \lambda>0
$$

#### Part A

To prove:

$$
\mathbb{E}[\mathbf{Z}^T\mathbf{MZ}] = \mu^T\mathbf{M}\mu +tr(\mathbf{M}\Sigma)
$$

We'll consider the hints where:

$$
\begin{gathered}
\mathbf{Z} = (\mathbf{Z} - \mu)+ \mu \\
\textrm{and} \\
\Sigma = \mathbb{E}[(\mathbf{Z}-\mu)(\mathbf{Z}-\mu)^T]
\end{gathered}
$$

Restating the problem we get a LHS of:

$$
\begin{gathered}
\mathbb{E}[\mathbf{Z}^T\mathbf{MZ}] \\
=\mathbb{E}\Big[\Big((\mathbf{Z}-\mu)+\mu\Big)^T\mathbf{M}\Big((\mathbf{Z}-\mu)+\mu\Big)\Big] \\
=\mathbb{E}\Big[\Big((\mathbf{Z}-\mu)^T+\mu^T\Big)\mathbf{M}\Big((\mathbf{Z}-\mu)+\mu\Big)\Big] \\
= \mathbb{E}\Big[\Big((\mathbf{Z}-\mu)^T\mathbf{M}+\mu^T\mathbf{M}\Big)\Big((\mathbf{Z}-\mu)+\mu\Big)\Big] \\
= \mathbb{E}\Big[
\underbrace{(\mathbf{Z}-\mu)^T\mathbf{M}(\mathbf{Z}-\mu)}_{\textrm{Term }1} + \underbrace{(\mathbf{Z}-\mu)^T\mathbf{M}\mu}_{\textrm{Term }2} +\underbrace{\mu^T\mathbf{M}(\mathbf{Z}-\mu)}_{\textrm{Term }3} +\underbrace{\mu^T\mathbf{M}\mu}_{\textrm{Term }4}
\Big]
\end{gathered}
$$

From here we can apply $\mathbb{E}[\cdot]$ to each term. We can tackle the middle terms (2,3) easily; Because $\mu$ and $\mathbf{M}$ are "constant, they'll be pulled out of the expectation. Also because $(\cdot)^T$ doesn't affect the averaging process, we can say that $\mathbb{E}[\cdot]^T = \mathbb{E}[(\cdot)^T]$. So simply,

$$
\mathbb{E}[(\mathbf{Z}-\mu)^T\mathbf{M}\mu] +\mathbb{E}[\mu^T\mathbf{M}(\mathbf{Z}-\mu)] \\
= \mathbb{E}[(\mathbf{Z}-\mu)^T]\mathbf{M}\mu +\mu^T\mathbf{M}\mathbb{E}[(\mathbf{Z}-\mu)]
$$

We can rewrite $\mathbb{E}[(\mathbf{Z}-\mu)^T]$ as $\mathbb{E}[(\mathbf{Z}-\mu)]^T$. However, $\mathbb{E}[(\mathbf{Z}-\mu)] = \mu -\mu =0$ so our result for the middle terms comes to $0$.

Next we're left with the first and fourth terms. Handling the fourth term is simple given that $\mathbb{E}[\mu^T\mathbf{M}\mu] =\mu^T\mathbf{M}\mu$

All that's left is the first term. Looking at dimensions, let $\mathbf{Z}$ have dimensions $n\times 1$ so that $\mathbf{Z}-\mu$ has the same dimensions. With this in mind:

$$
\underset{1\times n}{(\mathbf{Z}-\mu)^T}\ \underset{n\times n}{\mathbf{M}}\ \underset{n\times1}{(\mathbf{Z}-\mu)}=\underset{1\times1}{\mathbf{A}}
$$

The expectation of $\mathbf{A}$ would just be a scalar. Applying trace to a scalar just results in the scalar. So $\mathbb{E}[\textrm{tr}(\mathbf{A})] = \textrm{tr}(\mathbf{A}) = \mathbf{A}$. This justifies using $\textrm{tr}(\cdot)$ on the argument of the expectation. Another thing to consider is that $\textrm{tr}(\mathbf{ABC})=\textrm{tr}(\mathbf{BCA})$. Lastly, $\mathbb{E}[\textrm{tr}(\cdot)] = \textrm{tr}(\mathbb{E}[\cdot])$

$$
\begin{aligned}
\mathbb{E}\Big[\textrm{tr}\Big((\mathbf{Z}-\mu)^T\mathbf{M}(\mathbf{Z}-\mu)\Big)\Big] \\ =\mathbb{E}\Big[\textrm{tr}\Big(\mathbf{M}(\mathbf{Z}-\mu)(\mathbf{Z}-\mu)^T\Big)\Big] \\
= \textrm{tr}\Big(\mathbb{E}\big[\mathbf{M}(\mathbf{Z}-\mu)(\mathbf{Z}-\mu)^T\big]\Big) \\
= \textrm{tr}\Big(\mathbf{M}\mathbb{E}\big[(\mathbf{Z}-\mu)(\mathbf{Z}-\mu)^T\big]\Big) \\
\end{aligned}
$$

Remembering our hint, $\Sigma=\mathbb{E}\big[(\mathbf{Z}-\mu)(\mathbf{Z}-\mu)^T\big]$ our first term simplifies to:

$$
\textrm{tr}\big(\mathbf{M}\Sigma\big)
$$

So altogether we have simplified our LHS to equal the RHS:

$$
\overset{\checkmark}{\mu^T\mathbf{M}\mu + \textrm{tr}\big(\mathbf{M}\Sigma\big) = \mu^T\mathbf{M}\mu + \textrm{tr}\big(\mathbf{M}\Sigma\big)}
$$

#### Part B

Restating the problem, $Prove$:

$$
\begin{gathered}
\textrm{MSE}(\lambda) = \mathbb{E}\big[(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})^T
(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})\big] \\
=\boldsymbol{\beta}^T(\mathbf{I-M}(\lambda)\mathbf{X})^T(\mathbf{I-M}(\lambda)\mathbf{X})\boldsymbol{\beta} + \sigma^2\textrm{tr}(\mathbf{M}(\lambda)^T\mathbf{M}(\lambda)) \\
\textrm{where}\ \ \mathbf{M}(\lambda) = (\textbf{X}^T\textbf{X}+\lambda \mathbf{I})^{-1}\textbf{X}^T
\end{gathered}
$$

Looking at the first line above, we can let $\mathbf{Z} \equiv(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})$ from the previous identity proved in Part A. And not seen, is the $\mathbf{M}$ which just equals the identity matrix $\mathbf{I}$. So we can say:

$$
\begin{gathered}
\textrm{MSE}(\lambda) = \mathbb{E}[\mathbf{Z}^T\mathbf{MZ}] = \mu^T\mathbf{M}\mu +\textrm{tr}(\mathbf{M}\Sigma) \\
= \mu^T\mathbf{I}\mu +\textrm{tr}(\mathbf{I}\Sigma) = \mu^T\mu +\textrm{tr}(\Sigma)
\end{gathered}
$$

Finding $\mathbb{E}\big[\boldsymbol\beta-\hat{\boldsymbol\beta}\big]$ is simple if we substitute $Y = \mathbf{X}\beta+\epsilon$ and keep in mind that $\hat{\boldsymbol{\beta}}_{ridge} = (\mathbf{X}^T\mathbf{X}+\mathbf{I}\lambda)^{-1}Y = \mathbf{M}(\lambda)Y$:

$$
\begin{gathered}
\mathbb{E}\big[\boldsymbol\beta-\hat{\boldsymbol{\beta}}_{ridge}\big] \\
=\mathbb{E}\big[\boldsymbol\beta\big]-\mathbb{E}\big[\hat{\boldsymbol{\beta}}_{ridge}\big] \\
= \boldsymbol{\beta} - \mathbb{E}\big[\mathbf{M}(\lambda)Y\big] \\
= \boldsymbol{\beta} - \mathbb{E}\Big[\mathbf{M}(\lambda)\big(\mathbf{X}\boldsymbol{\beta}+\epsilon\big)\Big] \\
= \boldsymbol{\beta} - \mathbb{E}\Big[\mathbf{M}(\lambda)\mathbf{X}\boldsymbol{\beta}+\mathbf{M}(\lambda)\epsilon\Big] \\
\end{gathered}
$$

From here we can separate the expectation and we know that $\mathbf{M}(\lambda),\mathbf{X},\textrm{and}\  \boldsymbol{\beta}$ are "constant". This means their expectations are just themselves. However, for $\mathbb{E}[\mathbf{M}(\lambda)\epsilon] \rightarrow \mathbf{M}(\lambda)\mathbb{E}[\epsilon] =0$ because we're told that $\epsilon$ consists of uncorrelated, **mean zero** random variables. This greatly simplifies the expression derived above to:

$$
\begin{gathered}
\boldsymbol{\beta} - \mathbf{M}(\lambda)\mathbf{X}\boldsymbol{\beta} 
=\boxed{\big(\mathbf{I}-\mathbf{M}(\lambda)\mathbf{X}\big)\boldsymbol{\beta} = \mu}\\
\quad \\
\therefore \ \mu^T = \boldsymbol{\beta}^T\big(\mathbf{I}-\mathbf{M}(\lambda)\mathbf{X}\big)^T
\end{gathered}
$$

And to approach the $\textrm{tr}(\Sigma)$ we first have to break down what the covariance matrix of $\mathbf{Z} \equiv(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})$ is. Because we know that $\Sigma = \textrm{Var}(\mathbf{Z})$ we can rewrite this problem as:

$$
\begin{gathered}
\Sigma = \textrm{Var}\big((\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})\big) \\
= \textrm{Var}\big(\boldsymbol{\beta}-\mathbf{M}(\lambda)\mathbf{X}\boldsymbol{\beta}-\mathbf{M}(\lambda)\epsilon\big)
\end{gathered}
$$

To evaluate the variance, we'll establish $\mathbf{b} \equiv \boldsymbol{\beta}-\mathbf{M}(\lambda)\mathbf{X}\boldsymbol{\beta}$ and $A\mathbf{X} \equiv -\mathbf{M}(\lambda)\epsilon$. With that, we can apply variance rules such as $\textrm{Var}(A\mathbf{X}+\mathbf{b}) = A\textrm{Var}(\mathbf{X})A^T$. From here, $\textrm{Var}(\mathbf{X})=\textrm{Var}(\epsilon) = \sigma^2\mathbf{I}$. The $\mathbf{b}$ goes away and we're left with:

$$
\big(-\mathbf{M}(\lambda)\big)(\sigma^2\mathbf{I})\big(-\mathbf{M}(\lambda)\big)^T = \boxed{ \sigma^2\mathbf{M}(\lambda)\mathbf{M}(\lambda)^T = \Sigma}
$$ Lastly applying the trace to $\Sigma$ we can use its property of $\textrm{tr}(AB)=\textrm{tr}(BA)$ and move the constant out front:

$$
\textrm{tr}(\sigma^2\mathbf{M}(\lambda)\mathbf{M}(\lambda)^T) = \boxed{ \sigma^2\textrm{tr}\big(\mathbf{M}(\lambda)^T\mathbf{M}(\lambda)\big)= \textrm{tr}(\Sigma)}
$$

Putting it all together:

$$
\begin{gathered}
\textrm{MSE}(\lambda) = \mathbb{E}\big[(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})^T
(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})\big] \\
= \mu^T\mathbf{I}\mu +\textrm{tr}(\mathbf{I}\Sigma) = \mu^T\mu +\textrm{tr}(\Sigma) \\
= \boldsymbol{\beta}^T\big(\mathbf{I}-\mathbf{M}(\lambda)\mathbf{X}\big)^T \big(\mathbf{I}-\mathbf{M}(\lambda)\mathbf{X}\big)\boldsymbol{\beta} + 
\sigma^2\textrm{tr}\big(\mathbf{M}(\lambda)^T\mathbf{M}(\lambda)\big)\\
\blacksquare
\end{gathered}
$$

#### Part C

Finding the MSE for the OLS estimators just means evaluating $\mathbf{M}(0)$ and simplifying the rest of the expression. $\mathbf{M}(0)$ simply equals

$$
\mathbf{M}(0)=(\textbf{X}^T\textbf{X}+(0) \mathbf{I})^{-1}\textbf{X}^T = (\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T
$$ So from here we can sub $\mathbf{M}(0)$ into $\textrm{MSE}(\lambda)$

$$
\begin{gathered}
\textrm{MSE(0)}=\boldsymbol{\beta}^T\big(\mathbf{I}-\mathbf{M}(0)\mathbf{X}\big)^T \big(\mathbf{I}-\mathbf{M}(0)\mathbf{X}\big)\boldsymbol{\beta} + 
\sigma^2\textrm{tr}\big(\mathbf{M}(0)^T\mathbf{M}(0)\big) \\
= \boldsymbol{\beta}^T\big(\mathbf{I}-\big((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\big)\mathbf{X}\big)^T
\big(\mathbf{I}-\big((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\big)\mathbf{X}\big)\boldsymbol{\beta} + 
\sigma^2\textrm{tr}\Big(\big((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T
\big)^T\big((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\big)\Big)
\end{gathered}
$$

The $\big((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\big)\mathbf{X}$ part simplifies to $\mathbf{I}$ because it's a matrix multiplied by it's inverse. To the left of the addition sign simplifies easily:

$$
\begin{gathered}
\boldsymbol{\beta}^T\big(\mathbf{I}-\big((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\big)\mathbf{X}\big)^T
\big(\mathbf{I}-\big((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\big)\mathbf{X}\big)\boldsymbol{\beta} \\
= \boldsymbol{\beta}^T(\mathbf{I}-\mathbf{I})^T(\mathbf{I}-\mathbf{I})\boldsymbol{\beta} = 0
\end{gathered}
$$

The remaining expression for the $\textrm{MSE}(0)$ is much simpler and will require some manipulation to simplify entirely.

$$
\begin{gathered}
\textrm{MSE}(0) = \sigma^2\textrm{tr}\Big(\big((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T
\big)^T\big((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\big)\Big)\\
= \sigma^2\textrm{tr}\Big(\big(\textbf{X}(\textbf{X}^T\textbf{X})^{-1}
\big)\big((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\big)\Big)
\end{gathered}
$$

We'll let $\mathbf{A}=\mathbf{X}$ and $\mathbf{B}=(\textbf{X}^T\textbf{X})^{-1}(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T$ and use that $\textrm{tr}(AB)=\textrm{tr}(BA)$:

$$
\textrm{MSE}(0) =\sigma^2\textrm{tr}\Big((\textbf{X}^T\textbf{X})^{-1}
(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{X}\Big)
= \boxed{\sigma^2\textrm{tr}\Big((\textbf{X}^T\textbf{X})^{-1}
\mathbf{I}\Big)}
$$

#### Part D

With only one feature, our $\mathbf{M}(\lambda)$ changes to $\frac{\textbf{x}^T}{\sum x_i^2+\lambda}$ and $\textrm{MSE}(\lambda)$ changes to:

$$
\begin{gathered}
\textrm{MSE}(\lambda) = \boldsymbol{\beta}_1\big(1-\mathbf{M}(\lambda)\mathbf{X}\big)^T \big(1-\mathbf{M}(\lambda)\mathbf{X}\big)\boldsymbol{\beta}_1 + 
\sigma^2\textrm{tr}\big(\mathbf{M}(\lambda)^T\mathbf{M}(\lambda)\big) \\
= \boldsymbol{\beta}_1^2\bigg(1-\frac{\textbf{x}^T\mathbf{x}}{\sum x_i^2+\lambda}\bigg)^T
\bigg(1-\frac{\textbf{x}^T\mathbf{x}}{\sum x_i^2+\lambda}\bigg) + 
\sigma^2\textrm{tr}\bigg(\frac{\textbf{x}^T}{\sum x_i^2+\lambda}\Big(\frac{\textbf{x}^T}{\sum x_i^2+\lambda}\Big)^T\bigg) \\
= \boldsymbol{\beta}_1^2\bigg(\frac{\sum x_i^2+\lambda}{\sum x_i^2+\lambda}-\frac{\sum x_i^2}{\sum x_i^2+\lambda}\bigg)^2 +
\sigma^2\bigg(\frac{\textbf{x}^T}{\sum x_i^2+\lambda}\Big(\frac{\textbf{x}}{\sum x_i^2+\lambda}\Big)\bigg) \\
= \boldsymbol{\beta}_1^2\bigg(\frac{\lambda}{\sum x_i^2+\lambda}\bigg)^2+
\sigma^2\bigg(\frac{\sum x_i^2}{\big(\sum x_i^2+\lambda\big)^2}\bigg)\\
\boxed{\textrm{MSE}_{p=1}(\lambda)= \frac{\boldsymbol{\beta}_1^2\lambda^2+\sigma^2\sum x_i^2}{\big(\sum x_i^2+\lambda\big)^2}} \\
\quad \\
\boxed{\textrm{MSE}_{p=1}(0) = \frac{\sigma^2}{\sum x_i^2}}
\end{gathered}
$$

After looking at the two MSEs, as long as $\lambda$ follows the inequality below, ridge regression will have a lower MSE than standard OLS

$$
\boxed{\lambda < \frac{2\sigma^2\sum x_i^2}{(\sum x_i^2)\beta_1^2 - \sigma^2}}
$$
