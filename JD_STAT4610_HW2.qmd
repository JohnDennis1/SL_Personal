---
title: "JD_STAT4610_HW2"
author: "John Dennis"
format: pdf
---

### Problem 1

We're told that

$$
\hat{\beta}_{ridge} = (\textbf{X}^T\textbf{X}+\lambda \mathbf{I})^{-1}\textbf{X}^T\mathbf{Y}, \ \ \lambda>0
$$

#### Part A

To prove:

$$
\mathbb{E}[\mathbf{Z}^T\mathbf{MZ}] = \mu^T\mathbf{M}\mu +tr(\mathbf{M}\Sigma)
$$

We'll consider the hints where:

$$
\begin{gathered}
\mathbf{Z} = (\mathbf{Z} - \mu)+ \mu \\
\textrm{and} \\
\Sigma = \mathbb{E}[(\mathbf{Z}-\mu)(\mathbf{Z}-\mu)^T]
\end{gathered}
$$

Restating the problem we get a LHS of:

$$
\begin{gathered}
\mathbb{E}[\mathbf{Z}^T\mathbf{MZ}] \\
=\mathbb{E}\Big[\Big((\mathbf{Z}-\mu)+\mu\Big)^T\mathbf{M}\Big((\mathbf{Z}-\mu)+\mu\Big)\Big] \\
=\mathbb{E}\Big[\Big((\mathbf{Z}-\mu)^T+\mu^T\Big)\mathbf{M}\Big((\mathbf{Z}-\mu)+\mu\Big)\Big] \\
= \mathbb{E}\Big[\Big((\mathbf{Z}-\mu)^T\mathbf{M}+\mu^T\mathbf{M}\Big)\Big((\mathbf{Z}-\mu)+\mu\Big)\Big] \\
= \mathbb{E}\Big[
\underbrace{(\mathbf{Z}-\mu)^T\mathbf{M}(\mathbf{Z}-\mu)}_{\textrm{Term }1} + \underbrace{(\mathbf{Z}-\mu)^T\mathbf{M}\mu}_{\textrm{Term }2} +\underbrace{\mu^T\mathbf{M}(\mathbf{Z}-\mu)}_{\textrm{Term }3} +\underbrace{\mu^T\mathbf{M}\mu}_{\textrm{Term }4}
\Big]
\end{gathered}
$$

From here we can apply $\mathbb{E}[\cdot]$ to each term. We can tackle the middle terms (2,3) easily; Because $\mu$ and $\mathbf{M}$ are "constant, they'll be pulled out of the expectation. Also because $(\cdot)^T$ doesn't affect the averaging process, we can say that $\mathbb{E}[\cdot]^T = \mathbb{E}[(\cdot)^T]$. So simply,

$$
\mathbb{E}[(\mathbf{Z}-\mu)^T\mathbf{M}\mu] +\mathbb{E}[\mu^T\mathbf{M}(\mathbf{Z}-\mu)] \\
= \mathbb{E}[(\mathbf{Z}-\mu)^T]\mathbf{M}\mu +\mu^T\mathbf{M}\mathbb{E}[(\mathbf{Z}-\mu)]
$$

We can rewrite $\mathbb{E}[(\mathbf{Z}-\mu)^T]$ as $\mathbb{E}[(\mathbf{Z}-\mu)]^T$. However, $\mathbb{E}[(\mathbf{Z}-\mu)] = \mu -\mu =0$ so our result for the middle terms comes to $0$.

Next we're left with the first and fourth terms. Handling the fourth term is simple given that $\mathbb{E}[\mu^T\mathbf{M}\mu] =\mu^T\mathbf{M}\mu$

All that's left is the first term. Looking at dimensions, let $\mathbf{Z}$ have dimensions $n\times 1$ so that $\mathbf{Z}-\mu$ has the same dimensions. With this in mind:

$$
\underset{1\times n}{(\mathbf{Z}-\mu)^T}\ \underset{n\times n}{\mathbf{M}}\ \underset{n\times1}{(\mathbf{Z}-\mu)}=\underset{1\times1}{\mathbf{A}}
$$

The expectation of $\mathbf{A}$ would just be a scalar. Applying trace to a scalar just results in the scalar. So $\mathbb{E}[\textrm{tr}(\mathbf{A})] = \textrm{tr}(\mathbf{A}) = \mathbf{A}$. This justifies using $\textrm{tr}(\cdot)$ on the argument of the expectation. Another thing to consider is that $\textrm{tr}(\mathbf{ABC})=\textrm{tr}(\mathbf{BCA})$. Lastly, $\mathbb{E}[\textrm{tr}(\cdot)] = \textrm{tr}(\mathbb{E}[\cdot])$

$$
\begin{aligned}
\mathbb{E}\Big[\textrm{tr}\Big((\mathbf{Z}-\mu)^T\mathbf{M}(\mathbf{Z}-\mu)\Big)\Big] \\ =\mathbb{E}\Big[\textrm{tr}\Big(\mathbf{M}(\mathbf{Z}-\mu)(\mathbf{Z}-\mu)^T\Big)\Big] \\
= \textrm{tr}\Big(\mathbb{E}\big[\mathbf{M}(\mathbf{Z}-\mu)(\mathbf{Z}-\mu)^T\big]\Big) \\
= \textrm{tr}\Big(\mathbf{M}\mathbb{E}\big[(\mathbf{Z}-\mu)(\mathbf{Z}-\mu)^T\big]\Big) \\
\end{aligned}
$$

Remembering our hint, $\Sigma=\mathbb{E}\big[(\mathbf{Z}-\mu)(\mathbf{Z}-\mu)^T\big]$ our first term simplifies to:

$$
\textrm{tr}\big(\mathbf{M}\Sigma\big)
$$

So altogether we have simplified our LHS to equal the RHS:

$$
\overset{\checkmark}{\mu^T\mathbf{M}\mu + \textrm{tr}\big(\mathbf{M}\Sigma\big) = \mu^T\mathbf{M}\mu + \textrm{tr}\big(\mathbf{M}\Sigma\big)}
$$

#### Part B

Restating the problem, $Prove$:

$$
\begin{gathered}
\textrm{MSE}(\lambda) = \mathbb{E}\big[(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})^T
(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})\big] \\
=\boldsymbol{\beta}^T(\mathbf{I-M}(\lambda)\mathbf{X})^T(\mathbf{I-M}(\lambda)\mathbf{X})\boldsymbol{\beta} + \sigma^2\textrm{tr}(\mathbf{M}(\lambda)^T\mathbf{M}(\lambda)) \\
\textrm{where}\ \ \mathbf{M}(\lambda) = (\textbf{X}^T\textbf{X}+\lambda \mathbf{I})^{-1}\textbf{X}^T
\end{gathered}
$$

Looking at the first line above, we can let $\mathbf{Z} \equiv(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})$ from the previous identity proved in Part A. And not seen, is the $\mathbf{M}$ which just equals the identity matrix $\mathbf{I}$. So we can say:

$$
\begin{gathered}
\textrm{MSE}(\lambda) = \mathbb{E}[\mathbf{Z}^T\mathbf{MZ}] = \mu^T\mathbf{M}\mu +\textrm{tr}(\mathbf{M}\Sigma) \\
= \mu^T\mathbf{I}\mu +\textrm{tr}(\mathbf{I}\Sigma) = \mu^T\mu +\textrm{tr}(\Sigma)
\end{gathered}
$$

Finding $\mathbb{E}\big[\boldsymbol\beta-\hat{\boldsymbol\beta}\big]$ is simple if we substitute $Y = \mathbf{X}\beta+\epsilon$ and keep in mind that $\hat{\boldsymbol{\beta}}_{ridge} = (\mathbf{X}^T\mathbf{X}+\mathbf{I}\lambda)^{-1}Y = \mathbf{M}(\lambda)Y$:

$$
\begin{gathered}
\mathbb{E}\big[\boldsymbol\beta-\hat{\boldsymbol{\beta}}_{ridge}\big] \\
=\mathbb{E}\big[\boldsymbol\beta\big]-\mathbb{E}\big[\hat{\boldsymbol{\beta}}_{ridge}\big] \\
= \boldsymbol{\beta} - \mathbb{E}\big[\mathbf{M}(\lambda)Y\big] \\
= \boldsymbol{\beta} - \mathbb{E}\Big[\mathbf{M}(\lambda)\big(\mathbf{X}\boldsymbol{\beta}+\epsilon\big)\Big] \\
= \boldsymbol{\beta} - \mathbb{E}\Big[\mathbf{M}(\lambda)\mathbf{X}\boldsymbol{\beta}+\mathbf{M}(\lambda)\epsilon\Big] \\
\end{gathered}
$$

From here we can separate the expectation and we know that $\mathbf{M}(\lambda),\mathbf{X},\textrm{and}\  \boldsymbol{\beta}$ are "constant". This means their expectations are just themselves. However, for $\mathbb{E}[\mathbf{M}(\lambda)\epsilon] \rightarrow \mathbf{M}(\lambda)\mathbb{E}[\epsilon] =0$ because we're told that $\epsilon$ consists of uncorrelated, **mean zero** random variables. This greatly simplifies the expression derived above to:

$$
\begin{gathered}
\boldsymbol{\beta} - \mathbf{M}(\lambda)\mathbf{X}\boldsymbol{\beta} 
=\boxed{\big(\mathbf{I}-\mathbf{M}(\lambda)\mathbf{X}\big)\boldsymbol{\beta} = \mu}\\
\quad \\
\therefore \ \mu^T = \boldsymbol{\beta}^T\big(\mathbf{I}-\mathbf{M}(\lambda)\mathbf{X}\big)^T
\end{gathered}
$$

And to approach the $\textrm{tr}(\Sigma)$ we first have to break down what the covariance matrix of $\mathbf{Z} \equiv(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})$ is. Because we know that $\Sigma = \textrm{Var}(\mathbf{Z})$ we can rewrite this problem as:

$$
\begin{gathered}
\Sigma = \textrm{Var}\big((\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})\big) \\
= \textrm{Var}\big(\boldsymbol{\beta}-\mathbf{M}(\lambda)\mathbf{X}\boldsymbol{\beta}-\mathbf{M}(\lambda)\epsilon\big)
\end{gathered}
$$

To evaluate the variance, we'll establish $\mathbf{b} \equiv \boldsymbol{\beta}-\mathbf{M}(\lambda)\mathbf{X}\boldsymbol{\beta}$ and $A\mathbf{X} \equiv -\mathbf{M}(\lambda)\epsilon$. With that, we can apply variance rules such as $\textrm{Var}(A\mathbf{X}+\mathbf{b}) = A\textrm{Var}(\mathbf{X})A^T$. From here, $\textrm{Var}(\mathbf{X})=\textrm{Var}(\epsilon) = \sigma^2\mathbf{I}$. The $\mathbf{b}$ goes away and we're left with:

$$
\big(-\mathbf{M}(\lambda)\big)(\sigma^2\mathbf{I})\big(-\mathbf{M}(\lambda)\big)^T = \boxed{ \sigma^2\mathbf{M}(\lambda)\mathbf{M}(\lambda)^T = \Sigma}
$$ Lastly applying the trace to $\Sigma$ we can use its property of $\textrm{tr}(AB)=\textrm{tr}(BA)$ and move the constant out front:

$$
\textrm{tr}(\sigma^2\mathbf{M}(\lambda)\mathbf{M}(\lambda)^T) = \boxed{ \sigma^2\textrm{tr}\big(\mathbf{M}(\lambda)^T\mathbf{M}(\lambda)\big)= \textrm{tr}(\Sigma)}
$$

Putting it all together:

$$
\begin{gathered}
\textrm{MSE}(\lambda) = \mathbb{E}\big[(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})^T
(\boldsymbol{\beta}-\hat{\boldsymbol{\beta}}_{ridge})\big] \\
= \mu^T\mathbf{I}\mu +\textrm{tr}(\mathbf{I}\Sigma) = \mu^T\mu +\textrm{tr}(\Sigma) \\
= \boldsymbol{\beta}^T\big(\mathbf{I}-\mathbf{M}(\lambda)\mathbf{X}\big)^T \big(\mathbf{I}-\mathbf{M}(\lambda)\mathbf{X}\big)\boldsymbol{\beta} + 
\sigma^2\textrm{tr}\big(\mathbf{M}(\lambda)^T\mathbf{M}(\lambda)\big)\\
\blacksquare
\end{gathered}
$$

#### Part C

Finding the MSE for the OLS estimators just means

#### Part D

bullshit

### Problem 2
